X = input
y = target
m = length(y)
theta = parameter
lambda = regularization parameter
alpha = learning rate

%
% sigmoid function
% hypothesis for LogReg
%

g = 1 ./ (1 + exp(-z));

%
% LogReg cost function
%

h = sigmoid(X * theta);
J = (1/m) * ((-y' * log(h)) - (1-y)' * log(1 - h));
grad = (1/m) * (h - y)'*X;

% the prediction generated by the hypothesis
% are separated by the decision boundary
% here defined as >= 0.5

p = sigmoid(X * theta) >= 0.5



%
% LogReg Cost Regularized
%

h = sigmoid(X * theta);

% don't regularize thetaZero
noThetaZero = theta(2:length(theta));

% regularization term to balance out growth of theta
reg = (lambda / (2*m)) * noThetaZero' * noThetaZero

J = (1/m) * (-y' * log(h) - (1-y)' * log(1-h)) + reg


%
% Regularized Gradient Descent
%

% set thetaZero to zero (not regularized)
removeThetaZero = theta;
removeThetaZero(1) = 0;

grad = ((1 / m) * (h - y)' * X) + lambda / m * removeThetaZero'


